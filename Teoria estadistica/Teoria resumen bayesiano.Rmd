---
output:
  word_document: default
  html_document: default
---
## ¿Qué es estadística Bayesiana?

Para ver cómo el teorema de Bayes sirve para esto, primero debemos recordar qué es lo que nos dice dicho teorema. Recordemos que el teorema de Bayes establece que para dos eventos \(A\) y \(B\) se cumple

$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$

Con nuestro objetivo inferencial en mente entonces vamos a tener que el evento \(A\) representa la incertidumbre sobre \(\theta\) mientras que el evento \(B\) representa la información empírica brindada por los datos y las probabilidades de estos eventos están representadas por funciones de densidad (o probabilidad en el caso que los datos o el parámetro sea discreto). Por lo tanto, en notación Bayesiana podemos reescribir el Teorema de Bayes de la siguiente forma:

$$\pi(\theta|x) = \frac{\mathcal{L}(\theta|x)\pi(\theta)}{f(x)}$$

donde:

- \(\pi(\theta)\) representa la distribución *a priori*.

- \(\pi(\theta | x)\) representa la distribución *a posteriori*.

- \(\mathcal{L}(\theta)\) representa la función de verosimilitud de la muestra aleatoria.

- \(f(x)\) es la función de densidad/probabilidad de los datos, llamada a veces la constante Bayesiana o constante normalizadora.

Para entender mejor lo que significa esa constante normalizadora es buena idea retomar el Teorema de la Probabilidad Total. Este decía que si tenemos \(k\) eventos mutuamente excluyentes \(A_1 , A_2 , ... , A_k\) que tienen una intersección no nula con un evento \(B\) entonces se cumple

$$P(B) = P(B|A_1)P(A_1) + P(B|A_2)P(A_2) + ... + P(B|A_k)P(A_k)$$

De esta forma podríamos re-escribir el teorema de Bayes de la siguiente forma, para cualquier evento \(A_i\), \(i = 1, ... , k\)

$$P(A_i |B) = \frac{P(B|A_i)P(A_i)}{\sum_{i=1}^{k}P(B|A_i)P(A_i)}$$

Desde el punto de vista Bayesiano, el evento \(A_i\) representa un posible valor de \(\theta\), asumiendo que \(\theta\) sea discreto. Por lo tanto la constante normalizadora sería

$$f(x) = \sum_{i}^{k} \mathcal{L}(\theta_i|x)\pi(\theta_i)$$

En el caso donde \(\theta\) sea continua entonces cambiamos la sumatoria por una integral que abarque todo el dominio de \(\theta\)

$$f(x) = \int_{-\infty}^{+\infty} \mathcal{L}(\theta|x)\pi(\theta) d\theta$$

## Ejemplo sencillo

Vamos a asumir que nuestro parámetro desconocido solo puede tomar ciertos valores para ejemplificar los conceptos básicos que acabamos de ver.

- \(\theta\) es el número promedio de mascotas que tienen las personas en un edificio de apartamentos. 

- Supongamos que \(\theta\) solo puede tomar tres valores: 2, 2.5 y 3. De antemano vamos a creer que el valor más probable de esos tres es 2 (es decir, lo más probable es que las personas tengan en promedio 2 mascotas) ya que los apartamentos no son muy grandes. 

- Para propósitos del ejemplo diremos que \(P(\theta = 2) = 0.50\) mientras que los otros dos tienen probabilidades de \(0.25\).

- Podríamos pensar que el número de mascotas por apartamento sigue una distribución Poisson con media \(\theta\). Se preguntó en 5 apartamentos escogidos al azar cuántas mascotas tenían y los resultados fueron 1,4,3,2,3. Con base en esta información, actualice la información previa.

- Para llegar a la distribución de probabilidad a posteriori recordemos lo que nos dice el Teorema de Bayes para cuando el parámetro es discreto:

$$\pi(\theta_i |x) = \frac{\mathcal{L}(\theta_i |x)\pi(\theta_i)}{\sum_{i}^{3} \mathcal{L}(\theta_i|x)\pi(\theta_i)}$$

Nótese que necesitamos varios componentes para poder actualizar la información para cada uno de los tres valores de \(\theta\). Ya tenemos la función de probabilidad a priori pero todavía necesitamos la verosimilitud de una muestra aleatoria Poisson con \(n = 5\), la cual viene dada por

$$\mathcal{L}(\theta_i|x) = \prod_{j=1}^{5} \frac{\theta_{i}^{x_j} e^{-\theta_i} }{x_{j}!} = \frac{ \theta_{i}^{\sum_{j=1}^{5}x_j} e^{-5\theta_i} }{\prod_{j=1}^{5} x_{j}! }$$

Para el caso de la muestra que observamos tenemos que \(\sum_{j=1}^{5}x_j = 13\) y \(\prod_{j=1}^{5} x_{j}! = 1728\), por lo tanto

$$\mathcal{L}(\theta_i|x) =  \frac{ \theta_{i}^{13} e^{-5\theta_i} }{1728}$$

Ya con esta expresión podemos hacer una tabla donde vamos encontrando cada parte que necesitamos hasta llegar a la distribución a posteriori de \(\theta\):

| i | $\theta_i$ | $\pi(\theta_i)$ | $\mathcal{L}(\theta_i|x)$ | $\mathcal{L}(\theta_i|x)\pi(\theta_i)$ | $\pi(\theta_i| x)$ |
|--|------------|-----------------|------------------------------------|--------------------------------------|--------------------------| 
|1 |\( 2\)          | \(0.50\)            | \(2.15 \cdot 10^{-4}\)             | \(10.76\cdot 10^{-5}\)               | 0.416                     |
|2 |\(2.5\)        | \(0.25\)            | \(3.21 \cdot 10^{-4}\)             | \(8.03\cdot 10^{-5}\)                | 0.311                     |
|3 |\(3\)          | \(0.25\)            | \(2.82 \cdot 10^{-4}\)            | \(7.06\cdot 10^{-5}\)                | 0.273                     |


Nótese que para este caso tenemos que \(f(x) = \sum_{i}^{3} \mathcal{L}(\theta_i|x)\pi(\theta_i) = 10.76\cdot 10^{-5} + 8.03\cdot 10^{-5} + 7.06\cdot 10^{-5} = 2.58\cdot10^{-4}\). La probabilidad a posteriori se obtuvo dividiendo cada uno de los valores de esa suma entre el total.

## Relación entre el análisis Bayesiano y el frecuentista:

- Supongamos que queremos estimar el valor de \(\theta\) por medio de su estimador de máxima verosimilitud. Viendo la tabla vemos que la mayor verosimilitud sucede para \(\theta = 2.5\) por lo que ese sería su estimador máximo verosímil. 

- Sin embargo, si vemos cuál es el valor de \(\theta\) más probable, tanto a priori como a posteriori, podemos ver que sería el valor \(\theta = 2\).

- ¿Cuál fue el aporte Bayesiano al estudio? 

- Ya no consideramos tan probable como antes que el número promedio de mascotas sea de 2 por apartamento e incluso aumentaron las otras dos probabilidades, siendo el mayor aumento para la probabilidad de que \(\theta = 2.5\). Parece que los datos nos dieron una evidencia a favor de \(\theta = 2.5\), aunque haya sido leve.

Entre más informativa sea la priori, más se va a parecer la posteriori a esta. Por otro lado, si tenemos una gran cantidad de datos entonces la posteriori va a ser más similar a lo que aporta la verosimilitud. En otras palabras, con muchos datos, estos hablan por si solos, resultado análogo a la estadística frecuentista.

## Aclaración de parametrización
La función de densidad vendría dada de la siguiente manera
$$f_{X}(x) = \frac{ \beta^{\alpha} x^{\alpha - 1} e^{-\beta x}  }{ \Gamma(\alpha) } \quad si \quad x > 0$$
única diferencia es que el \(\beta\) la nueva parametrización, llamémoslo \(\beta^{\prime}\) es el inverso multiplicativo del beta de la parametrización vieja. Es decir, \(\beta^{\prime} = \frac{1}{\beta}\) Por lo tanto, para la nueva parametrización de la Gamma tenemos que E(X) = \(\frac{\alpha}{\beta}\) y \(Var(X) = \frac{\alpha}{\beta^2}\)

## Densidades previas conjugadas y estimadores de Bayes
    
### Distribución previa (distribución a priori)

Suponga que tenemos un modelo estadístico con parámetro \(\theta\). Su \(\theta\) es aleatorio entonces su densidad (antes de observar cualquier muestra) se llama **densidad previa**: \(\pi\).

**Ejemplo**: \(X_1,\dots, X_n \sim \text{Exp}(\theta)\) y \(\theta\) es aleatorio tal que \(\theta \sim \Gamma(\stackrel{\alpha}{1},\stackrel{\beta}{2})\) entonces

$$\pi(\theta) = \dfrac{1}{\Gamma(\alpha)}\beta^\alpha\theta^{\alpha-1}e^{\beta\theta} = 2e^{-2\theta}, \quad \theta > 0$$

*Ejemplo* (Componentes eléctricos) Supoga que se quiere conocer el tiempo de vida de cierto componente eléctrico. Sabemos que este tiempo se puede modelar con una distribución exponencial con parámetro θ desconocido. Este parámetro asumimos que tiene una distribución previa Gamma.
Un experto en componentes eléctricos conoce mucho de su área y sabe que el parámetro θ tiene las siguientes características:

$$\mathbb{E}[\theta] = 0.0002, \quad \sqrt{\text{Var}(\theta)} = 0.0001.$$

Como sabemos que la previa π es Gamma, podemos deducir lo siguiente:
$$\mathbb{E}[\theta] = \dfrac{\alpha}{\beta}, \text{Var}(\theta) = \dfrac{\alpha}{\beta^2}$$
$$\implies \begin{cases}\dfrac{\alpha}{\beta} = 2\times 10^{-4}\\\sqrt{\dfrac{\alpha}{\beta^2}} = 1 \times 10^{-4}\end{cases} \implies \beta = 20000, \alpha = 4$$

*Ejemplo*
Si X=(X1,…,Xn) es una muestra tal que Xi∼Exp(θ)
$$\begin{align*}
f_n(X|\theta) &= \begin{cases}\prod_{i=1}^n \theta e^{-\theta X_i} & \text{si } X_i>0\\
0 & \text{si no}
\end{cases}  \\
&= \begin{cases}\theta^n e^{-\theta\sum_{i=1}^n X_i} & X_i > 0  \\ 0 & \text{si no}\end{cases}
\end{align*}$$

## Densidad porterior 
Definición. Considere un modelo estadístico con parámetro θ y muestra aleatoria X1,…,Xn. La densidad condicional de θ dado X1,…,Xn se llama densidad posterior: π(θ|X)

$$\pi(\theta|X) =
\dfrac{f(X_1|\theta)\cdots f(X_n|\theta)\pi(\theta)}{g_n(X)}$$

para θ ∈ Ω, donde gn es una constante de normalización.

*Del ejemplo anterior*
$$f_n(X|\theta) = \theta^n e^{-\theta y}, y = \sum{X_i} \text{ (estadístico})$$

Numerador:
$$f_n(X|\theta)\pi(\theta) = \underbrace{\theta^n e^{-\theta y}}_{f_n(X|\theta)} \cdot \underbrace{\dfrac{200000^4}{3!}\theta^3e^{-20000\cdot\theta}}_{\pi(\theta)} = \dfrac{20000^4}{3!}\theta^{n+3}e^{(20000+y)\theta}$$
Denominador:
$$g_n(x) = \int_{0}^{+\infty}\theta^{n+3}e^{-(20000+y)\theta}\;d\theta = \dfrac{\Gamma(n+4)}{(20000+y)^{n+4}}$$

Entonces la posterior corresponde a
$$\pi(\theta|X) = \dfrac{\theta^{n+3}e^{-(20000+y)\theta}}{\Gamma(n+4)} (20000+y)^{n+4}$$

que es una Γ(n+4,20000+y)

con 5 observaciones (horas): 2911, 3403, 3237, 3509, 3118.
$$y = \sum_{i=1}^{5}X_i = 16178, \quad n= 5$$
por lo que \(\theta|X \sim \Gamma(9,36178)\)

## Proceso de modelación de parámetros

De ahora en adelante vamos a entender un modelo como el conjunto de los datos X1,…,Xn, la función de densidad f y el parámetro de la densidad θ. Estos dos últimos resumen el comportamiento de los datos.
Ahora para identificar este modelo se hace por partes,
La información previa π(θ) es la información extra o basado en la experiencia que tengo del modelo.
Los datos es la información observada. La función de densidad f filtra y mejora la información de la previa.
La densidad posterior es la "mezcla" entre la información y los datos observados. Es una versión más informada de la distribución del parámetro.

## Función de verosimilitud
Bajo el modelo estadístico anterior a fn(X|θ) se le llama verosimilitud o función de verosimilitud

Ejemplo.
Sea θ la proporción de aparatos defectuosos, con θ ∈ [0,1]
$$X_i = \begin{cases}0 & \text{falló} \\ 1 & \text{no falló} \end{cases}$$
\(\{X_i\}_{i=1}^n\) es una muestra aleatoria y \(X_i \sim Ber(\theta)\)

*Verosimilitud*
$$f_n(X|\theta) = \prod_{i=1}^n f(X_i|\theta) = \begin{cases}\theta^{\sum X_i}(1-\theta)^{n-\sum X_i} & X_i = 0,1~ \forall i \\ 0 & \text{si no}\end{cases}$$

*Previa:*
$$\pi(\theta) = 1_{\{0\leq\theta\leq 1\}}$$

*Posterior:*
Por el teorema de Bayes
$$\begin{align*}
\pi(\theta|X) \propto \theta^y (1-\theta)^{n-y}\cdot 1  \\
&= \theta^{\overbrace{y+1}^{\alpha}-1}(1-\theta)^{\overbrace{n-y+1}^{\beta}-1}
&\implies \theta|X \sim \text{Beta}(y+1,n-y+1)
\end{align*}$$

*Predicción.*
Supuesto: los datos son secuenciales. Calculamos la distribución posterior secuencialmente:
$$\begin{align}
\pi(\theta|X_1) & \propto \pi(\theta) f(X_1|\theta)\\
\pi(\theta|X_1,X_2) &\propto \pi(\theta) f(X_1,X_2|\theta) \\
&= \pi(\theta) f(X_1|\theta) f(X_2|\theta) \text{ (por independencia condicional)}
\\ & = \pi(\theta|X_1)f(X_2|\theta)\\
\vdots &  \\
\pi(\theta|X_1,\dots,X_n) & \propto f(X_n|\theta)\pi(\theta|X_1,\dots, X_{n-1})
\end{align}$$
Luego
$$\begin{align} g_n(X) & = \int_{\Omega} f(X_n|\theta) \pi(\theta|X_1,\dots, X_{n-1})~d\theta \\ & = P(X_n|X_1,\dots,X_{n-1}) \text{ (Predicción para }X_n) \end{align}$$

Bajo independencia condicional no hay diferencia en la posterior si los datos son secuenciales.

## Distribuciones previas



Ejemplo:

Podríamos usar una previa no informativa, de manera que 
θ∼Unif(0,B) donde B va a representar un valor arbitrario muy grande.
Ahora vamos a suponer que tenemos una muestra aleatoria X1,X2,...,Xn tal que Xj∼Poisson(θ), por lo que se cumple que  \(\mathcal{L}(\theta|x) = \frac{ \theta^{\sum_{j=1}^{n}x_j} e^{-n\theta} }{\prod_{j=1}^{n} x_{j}! }\) Para llegar a cuál sería la función de densidad a posteriori podemos utilizar el Teorema de Bayes:

$$\pi(\theta |x) = \frac{\mathcal{L}(\theta |x)\pi(\theta)}{\int_{0}^{+\infty} \mathcal{L}(\theta|x)\pi(\theta)}$$

Y dada la distribución a previa que escogimos entonces tenemos lo siguiente:
$$\mathcal{L}(\theta |x)\pi(\theta) = \frac{ \theta^{\sum_{j=1}^{n}x_j} e^{-n\theta} }{\prod_{j=1}^{n} x_{j}!} \cdot \frac{1}{B}$$

Este es el numerador de la expresión del Teorema de Bayes, pero por lo expresado anteriormente, para encontrar la posteriori, sabemos que π(θ|x)∝L(θ|x)π(θ)

Esto quiere decir que debemos encontrar el núcleo (o la parte variable) de esta expresión e identificar a qué función de densidad conocida pertenece. Por lo tanto,π(θ|x)∝L(θ|x)π(θ)
$$\Rightarrow \pi(\theta| x) \propto \frac{ \theta^{\sum x_j} e^{-n\theta} }{\prod x_{j}! } \cdot \frac{1}{B} \propto \theta^{n\bar{x}} e^{-n\theta}$$

Este es el núcleo de una distribución Gamma con \(\alpha = n\bar{x} + 1\) y \(\beta = n\). Por lo tanto, podemos decir que la distribución a posteriori para \(\theta\) es una \(Gamma(n\bar{x} + 1, n)\), denotado como \(\theta|x \sim Gamma(n\bar{x} + 1, n)\). Nótese como dato curioso que esta es una distribución centrada en \(\frac{n\bar{x} + 1}{n} = \bar{x} + \frac{1}{n}\), el cual es un valor muy cercano a \(\bar{x}\), especialmente con un \(n\) muy grande. 

Nosotros ya sabíamos que \(\bar{x}\) es el estimador de máxima verosimilitud para \(\theta\), por lo tanto si no tenemos mucha información sobre \(\theta\) a previa entonces tendría sentido basar nuestro conocimiento posterior alrededor de su estimador máximo verosímil. Esto nuevamente representa como el análisis Bayesiano con previas no informativas es muy similar al análisis frecuentista.

Siguiendo con el mismo ejemplo, supongamos que tenemos más información sobre nuestro parámetro 
θ, ¿cómo podríamos simularla? Sabemos que θ, al ser la media de una población Poisson, debe ser mayor a cero. Ya utilizamos la distribución Uniforme como una previa no informativa, por lo que sería contraproducente usarla cuando tenemos más información. Podríamos utilizar una distribución Gamma (o alguna de sus variaciones) ya que esta es para valores mayores que cero y además se pueden escoger parámetros α y β de manera que satisfagan nuestro conocimiento inicial sobre θ.
Este α y β llevan el nombre de hiperparámetros y podríamos definirlos como los parámetros de la distribución de un parámetro. Siguiendo el ejemplo podemos decir que inicialmente  \(\theta \sim Gamma(\alpha, \beta) \)Es decir
$$\pi(\theta) = \frac{ \beta^{\alpha} \theta^{\alpha - 1} e^{-\beta \theta}  }{ \Gamma(\alpha) }$$
Podemos proceder a encontrar la distribución posterior:

$$\pi(\theta|x) =  \frac{\mathcal{L}(\theta |x)\pi(\theta)}{\int_{0}^{+\infty} \mathcal{L}(\theta|x)\pi(\theta)} \propto \mathcal{L}(\theta |x)\pi(\theta) = \frac{ \theta^{n\bar{x}} e^{-n\theta} }{\prod x_{j}! } \cdot \frac{ \beta^{\alpha} \theta^{\alpha - 1} e^{-\beta \theta}  }{ \Gamma(\alpha) }$$
$$\propto \theta^{n\bar{x}} e^{-n\theta} \cdot \theta^{\alpha - 1} e^{-\beta \theta} = \theta^{n\bar{x} + \alpha - 1} e^{-\theta(n + \beta)}$$
Este es el núcleo de una Gamma con parámetros \(n\bar{x} + \alpha\) y \(n + \beta\). Por lo tanto concluimos que \(\theta|x \sim Gamma(n\bar{x} + \alpha , n + \beta)\). Solo con propósitos de comparación podemos observar que esta distribución tiene media \(\frac{n\bar{x} + \alpha}{n + \beta}\) la cual la podemos reescribir como \(\frac{n\bar{x} + \beta\left( \frac{\alpha}{\beta}\right)  }{n + \beta}\). Podemos ver que esta tiene la forma de un promedio ponderado de la media muestral \(\bar{x}\) y de la media a previa \(\frac{\alpha}{\beta}\). Entre mayor sea el tamaño de la muestra, mayor influencia van a tener los datos sobre la información posterior, mientras que si el \(\beta\) es más grande, entonces mayor influencia va a tener la previa sobre la posteriori. 

En estadística Bayesiana es posible modelar los hiperparámetros de la distribución de un parámetro. Por ejemplo, yo podría tratar el \(\alpha\) de este ejemplo como un parámetro desconocido al cual le puedo modelar la incertidumbre por medio de una distribución a previa, supongamos que por medio de otra Gamma con hiperparámetros \(\alpha^{\prime}\) y \(\beta^{\prime}\). Esto ocasiona que tengamos distintas etapas de previas en nuestro modelo:
	
$$X_1 , X_2 , ... , X_n \text{ t.q. }X_j \sim Poisson(\theta)$$
$$\theta|\alpha \sim Gamma(\alpha, \beta)$$
$$\alpha \sim Gamma(\alpha^{\prime}, \beta^{\prime})$$
Este tipo de modelo se denomina un **modelo Bayesiano jerárquico** y es el tipo de modelo más utilizando en la práctica pues tiene varias ventajas sobre algunos análisis frecuentistas. Sin embargo presentan un problema y es que en muy pocas ocasiones se puede llegar a un modelo a posteriori conocido por lo que se necesitan de simulaciones numéricas para poder hacer el análisis Bayesiano. 
	
Aunque existen muchos métodos hoy en día el más popular sigue siendo el método de cadenas de Markov Monte Carlo (MCMC) mediante muestreo de Gibbs. Dependiendo de la complejidad del modelo este requiere de mucha potencia computacional, por lo que estos modelos no eran muy utilizados en los inicios de la Estadística Bayesiana. Fue hasta la década de los 90s, donde la población en general tuvo mayor acceso a computadoras más poderosas, donde las técnicas Bayesianas empezaron a cobrar una mayor relevancia. 
	
Hasta el momento hemos hecho la selección de la previa un poco intuitivamente, sin embargo en la práctica la selección de la previa puede deberse a varios factores. 

## Estadística Bayesiana

El **teorema de Bayes** establece que para dos eventos \(A\) y \(B\) se cumple

$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$

**Estadística Bayesiana** considera \(\theta\) como un evento aleatorio y el evento \(X\) como la información empírica brindada por los datos:

$$\pi(\theta|x) = \frac{\mathcal{L}(\theta|x)\pi(\theta)}{f(x)}$$
**Teorema**. Bajo las condiciones anteriores: 

$$
\pi(\theta|X) =
\dfrac{f(X_1|\theta)\cdots f(X_n|\theta)\pi(\theta)}{g_n(X)} 
$$

para \(\theta \in \Omega\), donde \(g_n\) es una constante de normalización.

## Previas informativas
Una forma de seleccionar una previa es utilizando una previa informativa. Claramente, para poder hacer uso de esta debemos tener bastante conocimiento sobre el fenómeno de interés que estamos estudiando (como en el ejemplo que hicimos al inicio sobre la proporción de desempleo en el país). Para ello es imperativo tener información de varios expertos para poder llegar a construir el modelo estadístico que mejor represente esa información. Es importante destacar algo del uso de previas muy informativas; entre más informativa sea la previa que estamos utilizando entonces más datos se necesitan para tratar de observar algo nuevo. Imaginen que están casi 
100% seguros sobre cuánto debería ser el valor de cierto parámetro desconocido. Para que ustedes cambien de opinión entonces van a requerir de una gran cantidad de evidencia que apunte a lo contrario. Eso es lo que pasa si se usa una previa muy informativa; la posterior se va a parecer mucho a esta y solo podrá cambiar si existen muchos datos que apuntan a lo contrario.

## Familias conjugadas

**Definición**. Sea \(X_1,\dots, X_n\) i.i.d. condicional dado \(\theta\) con
densidad \(f(X|\theta)\). Sea \(\psi\) la familia de posibles densidades previas
sobre \(\Omega\). Si, sin importar los datos, la posterior pertenece a \(\psi\),
entonces decimos que \(\psi\) es una familia conjugada de previas.

**Ejemplos**:

* La familia Beta es familia conjugada para muestras según una Bernoulli.
* La familia Gama es familia conjugada para muestras exponenciales.
* Para el caso Poisson, si \(X_1,\dots,X_n\sim Poi(\lambda)\),entonces la familia
  Gamma es familia conjugada.

La función de densidad de una Poisson es \(P(X_i = k) = e^{-\lambda}\dfrac{\lambda^k}{k!}\). La verosimilitud corresponde a 
$$f_n(X|\lambda) = \prod_{i=1}^{n}e^{-\lambda}\dfrac{\lambda^{X_i}}{X_i!} = \dfrac{e^{-n\lambda}\lambda^Y}{\prod_{i=1}^n X_i!}~~~ \text{ donde } Y=\sum_{i=1}^n X_i.$$
La previa de \(\lambda\) está definida por \(\pi(\lambda)\propto\lambda^{\alpha-1}e^{-\beta\lambda}\). Por lo tanto, la posterior es
$$ \pi(\lambda|X) \propto \lambda^{Y+\alpha-1}e^{-(\beta+n)\lambda} \implies
 \lambda|X \sim \Gamma(Y+\alpha,\beta+n)$$ 

En el caso normal, si \(X_1,\dots,X_n\sim N(\theta,\sigma^2)\),entonces la familia normal es conjugada si \(\sigma^2\) es conocido.

Si \(\theta \sim N(\mu_0,V_0^2) \implies \theta|X \sim N(\mu_1, V_1^2)\) donde,
$$\mu_1 = \dfrac{\sigma^2\mu_0 + nV_0^2 \bar X_n}{\sigma^2 + nV_0^2}  = \dfrac{\sigma^2}{\sigma^2 + nV_0^2}\mu_0 + \dfrac{nV_0^2}{\sigma^2 + nV_0^2}\bar X_n$$

Combina de manera ponderada la previa y la de los datos.

**Ejemplo**

Considere una verosimilitud Poisson \((\lambda)\) y una previa

$$\pi(\lambda) = \begin{cases}2e^{-2\lambda} & \lambda> 0 \\ 0 & \lambda \leq 0\end{cases} \quad \lambda \sim \Gamma(1,2)$$

Supongamos que es una muestra aleatoria de tamaño \(n\). ¿Cuál es el número de observciones para reducir la varianza, a lo sumo, a 0.01?

Por teorema de Bayes, la posterior \(\lambda|x \sim \Gamma(y+1,n+2)\). Luego, la varianza de la Gamma es

$$\dfrac{\alpha}{\beta^2} = \dfrac{\sum x_i + 1}{(n+2)^2} \leq 0.01 \implies \dfrac{1}{(n+2)^2} \leq \dfrac{\sum x_i + 1}{(n+2)^2} \leq 0.01 \implies 100 \leq (n+2)^2 \implies n\geq 8$$

**Teorema**. Si \(X_1,\dots,X_n \sim N(\theta, \sigma^2)\) con \(\sigma^2\) conocido y la previa es \(\theta \sim N(\mu_0,V_0^2)\), entonces \(\theta|X\sim N(\mu_1,V_1^2)\) donde
$$ \mu_1 =  \dfrac{\sigma^2\mu_0 + nV_0^2 \bar X_n}{\sigma^2 + nV_0^2}, \quad V_1^2 = \dfrac{\sigma^2V_0^2}{\sigma^2 + nV_0^2}$$
*Prueba*:

**Verosimilitud**:

$$f_n(X|\theta) \propto \exp\left[- \dfrac{1}{2\sigma^2} \sum_{i=1}^{n}(X_i-\theta)^2\right]$$
Luego, 
$$\begin{align*}
\sum_{i=1}^n (X_i-\theta)^2 & = \sum_{i=1}^n (X_i-\bar X + \bar X - \theta)^2 \\
& = n(\bar X - \theta)^2 + \sum_{i=1}^n (X_i-\bar X)^2 + \underbrace{2 \sum_{i=1}^n (X_i-\bar X)(\bar X - \theta)}_{= 0 \text{ pues } \sum Xi = n\bar X)}
\end{align*}$$
Entonces
$$ f_n(X|\theta) \propto \exp\left[-\dfrac{n}{2\sigma ^2}(\bar X - \theta )^2\right].$$

**Previa**:

$$ \pi(\theta) \propto \exp\left[-\dfrac{1}{2V_0^2}(\theta - \mu_0)^2\right].$$

**Posterior**:

$$ \pi(\theta|X) \propto \exp\left[-\dfrac{n}{2\sigma ^2}(\bar X - \theta )^2-\dfrac{1}{2V_0^2}(\theta - \mu_0)^2\right].$$

Con \(\mu_1\) y \(V_1^2\) definidos anteriormente, se puede comprobar la siguiente identidad:

$$-\dfrac{n}{\sigma ^2}(\bar X - \theta )^2-\dfrac{1}{V_0^2}(\theta - \mu_0)^2= \dfrac{1}{V_1^2}(\theta-\mu_1)^2 + \underbrace{\dfrac{n}{\sigma^2 + nV_0^2}(\bar X_n- \mu_0)^2}_{\text{Constante con respecto a }\theta}$$
Por lo tanto, $$\pi(\theta|X) \propto \exp\left[-\dfrac{n}{2V_1^2}(\theta -\mu_1)^2\right]$$

*Media posterior*:

$$\mu_1 = \underbrace{\dfrac{\sigma^2}{\sigma^2 + nV_0^2}}_{W_1}\mu_0 + \underbrace{\dfrac{nV_0^2}{\sigma^2 + nV_0^2}}_{W_2}
\bar X_n$$

**Afirmaciones**:

1) Si \(V_0^2\) y \(\sigma^2\) son fijos, entonces \(W_1 \xrightarrow[n\to \infty]{}0\) (la importancia de la media empírica crece conforme aumenta \(n\)).

2) Si \(V_0^2\) y \(n\) son fijos, entonces \(W_2 \xrightarrow[\sigma^2\to \infty]{}0\) (la importancia de la media empírica decrece conforme la muestra es menos precisa).

3) Si \(\sigma^2\) y \(n\) son fijos, entonces \(W_2 \xrightarrow[V_0^2\to \infty]{}1\) (la importancia de la media empírica crece conforma la previa es menos precisa).

**Ejemplo (determinación de n)**

Sean \(X_1,\dots, X_n \sim N(\theta,1)\) y \(\theta\sim N(\mu_0,4)\). Sabemos que $$V_1^2 = \dfrac{\sigma^2V_0^2}{\sigma^2 + nV_0^2}. $$
Buscamos que \(V_1\leq 0.01\), entonces
$$ \dfrac{4}{4n+1}\leq 0.01 \implies n\geq 99.75 \text{ (al menos 100 observaciones)}$$

## Previas conjugadas
Un tipo de previas informativas que son sumamente convenientes de utilizar son las **previas conjugadas**. Se dice que una previa es conjugada si la distribución de la posteriori pertenece a la misma familia que la distribución de la previa. El ejemplo anterior donde utilizamos una previa Gamma y terminamos con una posteriori Gamma es un ejemplo de una previa conjugada. Este tipo de previas son muy convenientes pues nos aseguran que vamos a tener una distribución a posteriori conocida, lo único que cambiaría son los parámetros de la distribución. Sin embargo, esto no quiere decir que la Gamma siempre vaya a ser una previa conjugada; esto solo va a pasar si los datos son Poisson. Por lo tanto, una parte importante que permite que la previa sea o no sea conjugada es la distribución de la población.

Distribución             | previa conjugada                  |
-------------------------|-----------------------------------|
$$\text{Bernoulli}(p)$$    | $$p \sim Beta(\alpha,\beta)$$  |
$$\text{Binomial}(n,p)$$   | $$p \sim Beta(\alpha,\beta)$$  |
$$\text{Binomial Negativa}(n,p)$$| $$p \sim Beta(\alpha,\beta)$$|
$$\text{Poisson}(\lambda)$$      | $$\lambda \sim Gamma(\alpha,\beta)$$|
$$\text{Exponencial}(\theta)$$| $$\theta \sim Gamma(\alpha,\beta)$$|
$$\text{Normal}(\mu,\sigma^2)$$| $$\mu \sim N(\mu_0,\sigma^{2}_{0}) \quad \text{ y } \quad \sigma^{2} \sim GammaInversa(\alpha,\beta)$$|


## Previas no informativas
Este tipo de previa es posiblemente el más utilizado en la práctica. Con modelos relativamente simples utilizar una previa no informativa por lo general brinda resultados muy similares a los resultados frecuentistas, mientras que con modelos jerárquicos más complejos los resultados sí pueden ser más distintos. No obstante, la mayoría del tiempo en que se quiere hacer inferencia sobre parámetros desconocidos no se tiene mucha información al respecto, aparte de un posible rango en donde se puedan encontrar; por esto es más atractivo utilizar una previa no informativa. Diremos que una previa es no informativa si le da la libertad a los datos de encontrar los mejores valores de los parámetros.

Por mucho tiempo se tuvo la idea de que las previas no informativas eran exclusivamente las uniformes, pues asignaban probabilidades iguales a cualquier parámetro. Sin embargo, hay quienes criticaron esto, como Fisher, diciendo que no es posible que la uniforme sea siempre no informativa

## Previa de Jeffreys
Un tipo de previa no informativa es la **previa de Jeffreys**. Estas hacen uso de la Información de Fisher, previamente utilizada en el Tema 1. Por lo tanto, si tenemos una muestra aleatoria \(X_1 , X_2 , ... , X_n\) de una población con función de densidad \(f_{X}(x|\theta)\) entonces la información de Fisher se define como:
	
$$I(\theta) = -E\left[ \frac{\partial^{2} \ln(f_{X}(x|\theta)) }{\partial \theta^{2}} \right]$$
	
Por lo tanto, la previa de Jeffreys se define como:
	
$$\pi_{J}(\theta) = c\sqrt{I(\theta)}$$
	
Donde \(c\) es una constante positiva mayor a cero que asegura que esta función de densidad integra a uno. Como \(c\) es constante entonces podemos decir que \(\pi_{J}(\theta) \propto \sqrt{I(\theta)}\), por lo que muchas veces no nos importa el valor de \(c\) para encontrar la distribución a posteriori a partir de la previa de Jeffreys. 

Ejemplo: Sea \(X_1 , X_2 , ... , X_n\) una muestra aleatoria tal que \(X_j \sim N(\mu,1)\). Encuentre la previa de Jeffreys para \(\mu\). 
	
Solución: Recordemos la función de densidad de una Normal:
	
$$f_{X}(x|\mu) = \sqrt{2\pi} e^{-\frac{(x-\mu)^{2}}{2}}$$
	
Por lo que el logaritmo natural de esta sería:
	
$$\ln(f_{X}(x|\mu))  = \frac{1}{2}\ln(2\pi) - \frac{(x-\mu)^{2}}{2}$$
	
Derivamos dos veces con respecto a \(\mu\):
	
$$\frac{\partial \ln(f_{X}(x|\mu)) }{\partial \mu} = x - \mu$$
	
$$\Rightarrow \frac{\partial^{2} \ln(f_{X}(x|\mu)) }{\partial \mu^{2}} = -1$$

Finalmente obtenemos
	
$$I(\mu) = 1$$
	
Por lo tanto, podemos concluir que \(\pi_{J}(\theta) \propto 1\), es decir, es proporcional a una constante. Por lo tanto, la previa de Jeffreys para estimar a \(\mu\) sería una distribución Uniforme, escogida en un rango bastante amplio. 

Hay un par de puntos importantes de destacar cuando se usa la previa de Jeffreys. El primero de ellos es que no siempre se va a llegar a una función de densidad propia (es decir, una función de densidad que integre a 1 en su dominio). Por lo general se ignora este problema si la posteriori si es propia. Por lo tanto, siempre que se vaya a utilizar una previa impropia hay que revisar que la posteriori sea propia, si no los resultados no tendrían sentido. El segundo punto es un poco más filosófico y trata con el hecho de que las previas se deben elegir antes de ver los datos. La previa de Jeffreys usa la distribución de los datos para encontrar una previa lo que contradice lo que muchos dicen sobre el planteamiento de la previa. 

## Densidades previas impropias

**Definición**. Sea \(\pi\) una función positiva cuyo dominio está en \(\Omega\). Suponga que \(\int\pi(\theta)\;d\theta = \infty\). Entonces decimos que \(\pi\) es una **densidad impropia**.

**Ejemplo**: \(\theta \sim \text{Unif}(\mathbb{R})\), \(\lambda \sim \text{Unif}(0,\infty)\).

Una técnica para seleccionar distribuciones impropia es sustituir los hiperparámetros previos por 0.

**Ejemplo**:

Se presenta el número de soldados prusianos muertos por una patada de caballo (280 conteos, unidades de combate en 20 años).

| Unidades | Ocurrencias |  
|----------|-------------|
| 144      | 0           |   
|    91    | 1           |  
|    32    | 2           |   
|       11 | 3           |  
|         2| 4           |  

* Muestra de Poisson: \(X_1 = 0, X_2 = 1, X_3 = 1,\dots, X_{280} = 0 \sim \text{Poi}(\lambda)\).

* Previa: \(\lambda \sim \Gamma(\alpha, \beta)\).

* Posterior: \(\lambda|X \sim \Gamma(y+\alpha, n+\beta) = \Gamma(196 + \alpha, 280 + \beta)\).

Sustituyendo, \(\alpha=\beta = 0\)
$$\begin{align*}
\pi(\lambda) &= \dfrac{1}{\Gamma(\alpha)}\beta^\alpha\lambda^{\alpha-1}e^{\beta\lambda}  \\
& \propto \lambda^{\alpha-1}e^{-\lambda\beta} \\
&=\dfrac{1}{\lambda}
\end{align*}$$

Por teorema de Bayes, $$\theta|X \sim \Gamma(196,280)$$

## grafico 
```{r}

alfa1 <- 196 
beta1 <- 280
theta <- seq(0,2,by=0.01)
posterior <- dgamma(theta,shape = alfa1, rate = beta1)
plot(theta,posterior, type="l")
abline(v=196/280) # media a posteriori = media muestral
```

## Inferencia Bayesiana


Como en la estadística Bayesiana la posteriori es la distribución que contiene toda la información pertinente sobre \(\theta\), los problemas de inferencia estadística se solucionan trabajando con esta distribución. A continuación veremos de manera general los temas del curso pero dentro del enfoque Bayesiano. 

### Estimación puntual
	
En estadística Bayesiana se tiene el inconveniente de que tenemos incertidumbre sobre el parámetro y hay que ver cómo se puede sacar un valor puntual para \(\theta\) de toda esta incertidumbre. La respuesta tradicional es buscar el centro de la distribución a posteriori de \(\theta\). Por lo tanto, el **estimador de Bayes** más tradicional se define como la media de la distribución a posteriori de \(\theta\). Es decir, \(\hat{\theta}_{B} = E(\theta|x)\). 

En los ejemplos que vimos anteriormente donde encontrábamos la distribución a posteriori y calculamos los valores esperados de esta, lo que realmente estábamos haciendo era obteniendo el estimador de Bayes. A estos estimadores se le pueden volver a estudiar todas las propiedades que ya vimos en el curso (Insesgado, Eficiente, Consistente y Suficiente) e incluso comparar contra otros estimadores frecuentistas. 
	
Por lo general, los estimadores puntuales Bayesianos están asociados a una **función de pérdida**, denotada como \(\ell(\hat{\theta},\theta)\), que mide la penalización en la que se incurre al utilizar \(\hat{\theta}\) para estimar \(\theta\). La función de pérdida más común es la de pérdida cuadrática que se define como \(\ell(\hat{\theta},\theta) = (\hat{\theta}-\theta)^{2}\). 

El objetivo es encontrar el valor de \(\hat{\theta}\) que minimice \(E\left( \ell(\hat{\theta},\theta) \,\middle|\, x \right) = \int \ell(\hat{\theta},\theta) \pi(\theta|x) d\theta\), llamado el riesgo Bayesiano a posteriori, i.e.


$$\arg\min_{\hat{\theta}} = E\left( \ell(\hat{\theta},\theta) \,\middle|\, x \right)$$



Por lo tanto, podemos ver que el estimador de Bayes que minimiza el riesgo Bayesiano utilizando la función de pérdida cuadrática se obtiene minimizando la expresión
	
$$\int (\hat{\theta}-\theta)^{2} \pi(\theta|x) d\theta = E\left( (\hat{\theta}-\theta)^{2} \,\middle|\, x \right)$$
	
Encontremos el valor de \(\hat{\theta}\) que minimiza esta expresión. Primero simplifiquemos la expresión anterior un poco:
	
$$E\left( (\hat{\theta}-\theta)^{2} \,\middle|\, x \right) = E\left( \hat{\theta}^{2} -2\theta\hat{\theta} +\theta^{2} \,\middle|\, x \right) = \hat{\theta}^{2} -2\hat{\theta}E(\theta|x) + E(\theta^{2}|x)$$

Derivamos esta expresión con respecto a \(\hat{\theta}\):
	
$$\frac{\partial }{\partial \hat{\theta}}  E\left( (\hat{\theta}-\theta)^{2} \,\middle|\, x \right) = 2\hat{\theta} - 2E(\theta|x) = 0$$
$$\Rightarrow \hat{\theta} = E(\theta|x)$$
	
Como la segunda derivada es positiva entonces tenemos que \(\hat{\theta} = E(\theta|x)\) es el valor que minimiza el riesgo de Bayes con pérdida cuadrática. Este valor es el mismo al estimador de Bayes usualmente utilizando, por lo que podríamos decir que siempre que usemos la media a posteriori como estimador de \(\theta\) estamos usando el valor que minimiza el riesgo de Bayes con pérdida cuadrática. Si cambiamos la función de pérdida entonces el estimador de Bayes cambiaría. 

- **Función de pérdida absoluta** es \(\ell(\hat{\theta},\theta) = |\hat{\theta}-\theta)|\). Se puede demostrar que para este caso: 

$$\arg\min_{\hat{\theta}} = E\left( \ell(\hat{\theta},\theta) \,\middle|\, x \right) = E\left( |\hat{\theta}-\theta)| \,\middle|\, x \right)= Mediana(\theta|x)$$
Es decir, \(\hat{\theta}\) es el valor que satisface

$$F_{\theta|x}(\hat{\theta})=\frac{1}{2}.$$



### Otras funciones de pérdida

* \(l(\theta,a) = |\theta-a|^k\), \(k\ne 1,2\), \(0<k<1\).

* \(l(\theta,a) = \lambda(\theta)|\theta-a|^2\) \((\lambda(\theta)\) penaliza la magnitud del parámetro).

* \(l(\theta,a)=\begin{cases}3(\theta-a)^2 & \theta\leq a \text{ (sobreestima)}\\ (\theta-a)^2&\theta\geq a \text{ (subestima)} \end{cases}\)
	
## Intervalos de credibilidad
	
El análogo Bayesiano a los intervalos de confianza se llama intervalos de credibilidad (o de veracidad). Recordemos que en la estadística frecuentista la interpretación de un intervalo de confianza se hace tratando al intervalo como variable aleatoria, por lo que es incorrecto decir que \(\theta\) se encuentra entre tanto y tanto con tanta probabilidad. Sin embargo, en el punto de vista Bayesiano si es posible hacer una conclusión como esa pues tenemos todo un modelo probabilístico a posteriori sobre los posibles valores que podría tomar \(\theta\). 
	
Existen dos formas usualmente utilizadas para obtener intervalos de credibilidad. Nosotros nos estaremos enfocando más en generar intervalos del primer tipo ya que los de la segunda forma son más complejos de generar en papel y lápiz.

* **Intervalos de credibilidad de colas iguales**. Estos son posiblemente los más utilizados en la práctica. Son intervalos de la forma \(\left[a,b \right]\) de probabilidad \(1-\alpha\), donde \(a\) y \(b\) se definen mediante las ecuaciones
		
$$P(a < \theta < b|x) = 1-\alpha$$
$$P(\theta \leq a|x) = P(\theta \geq b |x) = \frac{\alpha}{2}$$ 
		
Por lo general están centrados alrededor de la media (si la distribución a posteriori es bastante simétrica). También existen las versiones unilaterales. 
		
* **Intervalos de credibilidad de máxima densidad a posteriori**. Se conocen como intervalos HPD por sus siglas en inglés. Su definición viene dada para un valor de corte \(c > 0\) tal que 
		
$$H(c) = \left\lbrace \theta : \pi(\theta|x) \geq c  \right\rbrace$$
		
El valor \(c\) se encuentra de forma que \(P(\theta \in H(c) | x) = 1- \alpha\). Encontrar este tipo de intervalos equivale a dibujar líneas horizontales en la densidad a posteriori a partir de la moda e ir bajando la línea horizontal (el valor de \(c\)) hasta que por debajo de esa línea se acumule una probabilidad de \(1-\alpha\). 

Veremos un ejemplo para in intervalo de credibilidad de colas iguales: 
	
Ejemplo: Sea \(X_1 , X_2 , ... , X_n\) una muestra aleatoria tal que \(X_j \sim N(\mu,\sigma^2)\), donde \(\sigma^{2}\) es conocido. Obtenga un intervalo de credibilidad de colas iguales para \(\mu\) de probabilidad \(1-\alpha\) utilizando como priori una distribución Uniforme. 
	
Solución: Lo primero que tenemos que hacer es encontrar la distribución a posteriori para \(\mu\). Como la priori es uniforme entonces tenemos que 
	
$$\pi(\mu|x) \propto \mathcal{L}(\mu|x) = (2\pi)^{-\frac{n}{2}} (\sigma^2)^{-\frac{n}{2}} e^{-\frac{\sum (x_j - \mu)^{2} }{2\sigma^2}} \propto e^{-\frac{\sum (x_j - \mu)^{2} }{2\sigma^2}}$$
	
Trabajando esta expresión un poco tenemos que
	
$$e^{-\frac{\sum (x_j - \mu)^{2} }{2\sigma^2}} = e^{ -\frac{\sum (x_j - \overline{x})^{2} + n(\overline{x} - \mu)^{2}}{2\sigma^2}  }  = e^{ -\frac{\sum (x_j - \overline{x})^{2}}{2\sigma^2}  }e^{ -\frac{ n(\overline{x} - \mu)^{2}}{2\sigma^2}  }  \propto e^{ -\frac{ n(\overline{x} - \mu)^{2}}{2\sigma^2}  }$$

Esta expresión se puede reescribir de la forma \(e^{ -\frac{ n(\mu - \overline{x})^{2}}{2\sigma^2}  }\). Este es el núcleo de una distribución Normal para \(\mu\) con media \(\overline{x}\) y variancia \(\frac{\sigma^2}{n}\). Por lo tanto tenemos que
$$\mu|x \sim N\left( \overline{x}, \frac{\sigma^2}{n} \right)$$

Ahora podemos proceder a encontrar el intervalo de credibilidad. Tenemos que encontrar valores \(a\) y \(b\) tales que \(P(a < \mu < b|x) = 1-\alpha\) y \(P(\mu \leq a|x) = P(\mu \geq b |x) = \frac{\alpha}{2}\). Por lo tanto
	
$$P(\mu \leq a|x) = P\left( \frac{\mu - \overline{x}}{\frac{\sigma}{\sqrt{n}}} \leq \frac{a - \overline{x}}{\frac{\sigma}{\sqrt{n}}} \,\middle|\, x \right)  = P\left( Z \leq \frac{\sqrt{n}(a - \overline{x}) }{\sigma} \,\middle|\, x \right)  = \frac{\alpha}{2}$$
	
Esto quiere decir que tenemos que buscar el cuantil de una normal estándar que acumula \(\frac{\alpha}{2}\) en su cola derecha. Este valor lo denotamos como \(z_{\frac{\alpha}{2}}\) que por simetría es igual a \(-z_{1-\frac{\alpha}{2}}\). Por lo tanto 
	
$$\frac{\sqrt{n}(a - \overline{x}) }{\sigma} = -z_{1-\frac{\alpha}{2}}$$
	
$$\Rightarrow a =  \overline{x} -z_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}$$
	
Si hacemos este procedimiento para encontrar \(b\) vamos a obtener que \(b = \overline{x} + z_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}\). Por lo tanto vamos a tener una probabilidad de \(1-\alpha\) de encontrar el verdadero valor de \(\mu\) en el intervalo \(\overline{x} \pm z_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}\). Vean que este intervalo es completamente idéntico al intervalo de confianza para \(\mu\) frecuentista; inclusive el estimador de Bayes es igual al estimador de máxima verosimilitud para \(\mu\). Esto se debe a que se utilizó una priori no informativa para obtener la posteriori, por lo que el resultado a posteriori es muy similar al resultado frecuentista. No obstante, el ejemplo se hizo con propósitos ilustrativos de cómo encontrar un intervalo de credibilidad con colas iguales.

	
## Factor de Bayes
	
Recordemos que nosotros tenemos las hipótesis \(H_{0}: \theta \in \Omega_{0}\) contra  \(H_{1}: \theta \in \Omega_{1}\). En la estadística frecuentista es imposible calcular las probabilidades de estas hipótesis por lo que tenemos que generar un contraste a partir de ciertos criterios para las probabilidades de Error Tipo I y de Error Tipo II. Sin embargo, en el caso Bayesiano donde tenemos una distribución a posteriori para \(\theta\) entonces sí es posible calcular las probabilidades de que \(H_0\) o \(H_1\) sean ciertos. Es decir, calcularíamos \(\alpha_{0} = P(\theta \in \Omega_{0} | x)\) y \(\alpha_{1} = P(\theta \in \Omega_{1} | x)\). 
	
Por lo tanto un contraste de hipótesis Bayesiano usualmente se reduce a comparar estas probabilidades y concluir correspondientemente. Podríamos decir que un Bayesiano va a rechazar la hipótesis nula si \(\alpha_0 <  0.5\). Aunque es posible obtener la potencia de este contraste, por lo general en estadística Bayesiana no nos preocupamos por estas medidas. Aunque este es el método usual de comparar hipótesis, algunos prefieren hacer una comparación de odds a priori y a posteriori, lo cual produce un contraste un poco distinto. Para esta explicación denotará las probabilidades a priori de \(H_0\) y \(H_1\) como \(\pi_0\) y \(\pi_1\), respectivamente. Es decir, \(\pi_0 = P(\theta \in \Omega_{0})\) y \(\pi_1 = P(\theta \in \Omega_{1})\). 

La cantidad \(\frac{\alpha_{0}}{\alpha_{1}}\) se define como los *odds* a posteriori de \(H_0\) con respecto a \(H_1\). Por otro lado podemos definir \(\frac{\pi_{0}}{\pi_{1}}\) como los *odds* a priori. Por lo tanto podemos definir la medida 
	
$$B = \frac{\frac{\alpha_{0}}{\alpha_{1}}}{\frac{\pi_{0}}{\pi_{1}}} = \frac{\alpha_{0}\pi_{1}}{\alpha_{1}pi_{0}}$$
	
como los *odds ratio* a favor de \(H_0\). Este valor se denomina el **factor de Bayes**. En ocasiones este factor se interpreta como "los odds de \(H_0\) con respecto a \(H_1\) dados por los datos".  Esta interpretación es válida cuando las hipótesis son simples es decir, cuando \(\Omega_{0} = \left\lbrace \theta_0\right\rbrace\) y \(\Omega_{1} = \left\lbrace \theta_1\right\rbrace\). En este caso tenemos que 
	
$$\alpha_{0} = P(\theta = \theta_0 |x) = \frac{\mathcal{L}(\theta_0|x)\pi_0}{\mathcal{L}(\theta_0|x)\pi_0 + \mathcal{L}(\theta_1|x)\pi_1}$$
	
$$\alpha_{1} = P(\theta = \theta_1 |x) = \frac{\mathcal{L}(\theta_1|x)\pi_1}{\mathcal{L}(\theta_0|x)\pi_0 + \mathcal{L}(\theta_1|x)\pi_1}$$

Por lo tanto:
	
$$\frac{\alpha_{0}}{\alpha_{1}} = \frac{\mathcal{L}(\theta_0|x)\pi_0}{\mathcal{L}(\theta_1|x)\pi_1}$$
	
$$\Rightarrow B = \frac{\alpha_{0}\pi_{1}}{\alpha_{1}pi_{0}} = \frac{\mathcal{L}(\theta_0|x)}{\mathcal{L}(\theta_1|x)} = \lambda$$
	
En otras palabras, \(B\) equivale a la razón de verosimilitudes de \(H_0\) con respecto a \(H_1\), la cual usualmente es interpretada como los odds de \(H_0\) con respecto a \(H_1\) dados por los datos, incluso por estadísticos frecuentistas. Sin embargo, cuando las hipótesis no son simples resulta que el factor de Bayes es distinto al estadístico de la razón de verosimilitudes pues este cubre todos los posibles valores de \(\theta\) en \(\Omega_{0}\) y \(\Omega_{1}\) en lugar de solo tomar en cuenta sus estimadores máximo verosímiles.
	
Si \(B > 1\) esto indica que \(H_0\) es más apoyado por los datos que \(H_1\) mientras que si \(B < 1\) entonces los datos apoyan más a \(H_1\) que a \(H_0\). El Cuadro 2 presenta la escala que desarrolló Jeffreys, enmendada por Kass y Raftery, para medir la magnitud del apoyo a \(H_0\) o \(H_1\).

Escala de evidencia de Jeffreys para el factor de Bayes

Factor de Bayes    | Interpretación                  |
-------------------|---------------------------------|
\(B < 1/150\)        |  Evidencia muy fuerte para \(H_1\)|
\(1/150 l< B < 1/20\) |  Evidencia fuerte para \(H_1\)    |
\(1/20 < B < 1/3\)   |  Evidencia moderada para \(H_1\)  |
\(1/3 < B < 1\)      |  Evidencia débil para \(H_1\)     | 
\(1 < B < 3\)        |  Evidencia débil para \(H_0\)     |
\(3 < B < 20\)       |  Evidencia moderada para \(H_0\)  |
\(20 < B < 150\)     |  Evidencia fuerte para \(H_0\)    |
\(B > 150\)          |  Evidencia muy fuerte para \(H_0\)|

---

Ejemplo: Retornemos al ejemplo de los apartamentos donde \(\theta\) solo podía tomar los valores 2, 2.5 y 3. Supongamos que se quiere contrastar las hipótesis \(H_0: \theta = 2\) contra \(H_1: \theta > 2\). 

Solución: Como ya para ese ejemplo obtuvimos la función de probabilidad a posteriori entonces podemos calcular las probabilidades de cada hipótesis:
	
$$\alpha_0 = P(\theta = 2|x) = 0.416$$
$$\alpha_1 = P(\theta > 2 | x) = P(\theta = 2.5|x) + P(\theta = 3|x) = 0.311 + 0.273 = 0.584 = 1 - \alpha_0$$ 

Comparando únicamente estos dos valores vemos que preferimos a \(H_1\) sobre \(H_0\). Si fuésemos a calcular el factor de Bayes entonces vamos a necesitar los odds a priori a posteriori. Estos vienen dados por
	
$$\frac{\pi_{0}}{\pi_{1}} = \frac{\pi_0}{1-\pi_0} = \frac{0.50}{0.50} = 1$$

$$\frac{\alpha_{0}}{\alpha_{1}} = \frac{\alpha_{0}}{1-\alpha_{0}} = \frac{0.416}{0.584} = 0.713$$
	
$$\Rightarrow B = \frac{0.713}{1} = 0.713$$

Es decir, tenemos que los odds de \(H_0\) a posteriori disminuyeron un \(28.7\%\) al pasar de la información a priori a la información a posteriori. Es decir, nuestros datos brindaron evidencia a favor de la hipótesis alterna. Viendo la escala del Cuadro 2 vemos que en este caso tenemos evidencia débil a favor de \(H_1\). 


Ejemplo: Sea \(X_1 , X_2 , X_3 , X_4\) una muestra aleatoria tal que \(X_j \sim N(\mu, 1)\). Suponiendo que se utilizó una priori Uniforme(0,30) para \(\mu\) y quieren contrastar las hipótesis \(H_{0}: \mu \geq 14\) contra \(H_{1}: \mu < 14\), encuentre el factor de Bayes. Suponga que de la muestra se obtuvo que \(\overline{x} = 15.3\). 
	
Solución: Sabemos que con esta priori entonces \(\mu|x \sim N\left( \overline{x},\frac{1}{n}\right)\), por lo tanto con los datos que observamos de la muestra tenemos que
	
$$\mu|x \sim N(15.3; 0.25 )$$
	
Ya que conocemos la priori y la posteriori de \(\mu\) debemos calcular las probabilidades de las hipótesis a prioi y a posteriori. Como la priori es una uniforme entonces tenemos que \(\pi_0 = P(\mu \geq 14) = \frac{30-14}{30} = 0.533\). Por lo tanto se cumple que \(\pi_1 = P(\mu < 14) = 1 - \pi_0 = 0.467\). Por lo tanto, los odds a priori son \(\frac{0.533}{0.467} = 1.143\). 

Ahora calculamos las probabilidades a posteriori. Como la distribución a posteriori es una Normal vamos a tener que estandarizar para poder calcular probabilidades mediante la tabla de la normal estándar (o usar la calculadora o R). Por lo tanto tenemos que la probabilidad a posteriori de \(H_0\) viene dada por \(\alpha_{0} = P(\mu \geq 14 | x) = P\left( Z \geq 2(14-15.3) | x \right) = 1 - P(Z < -2.6) = 0.9953\). Por consiguiente tenemos que \(\alpha_{1} = 0.0047\) y los odds a posteriori son \(213.5376\).
	 
Esto significa que el factor de Bayes sería \(\frac{213.5376}{1.143} = 186.8454\) el cual, según la escala de Jeffreys, muestra evidencia muy fuerte a favor de \(H_0\). 




